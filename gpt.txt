import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
import gc
import optuna
import time
from datetime import timedelta
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit

def clean_column_to_int(series, false_values=('Нет', 'False', 'false', '-', '', None), true_values=('Да', 'True', 'true')):
    ser = series.copy()
    ser = ser.astype(str).str.strip().str.lower()
    ser = ser.replace(list(false_values), 0, regex=False)
    ser = ser.replace(list(true_values), 1, regex=False)
    ser = ser.infer_objects(copy=False)
    ser = pd.to_numeric(ser, errors='coerce').fillna(0).astype('int8')
    return ser

def load_data():
    print("\n=== DEBUG: Начало загрузки данных ===")
    t0 = time.time()
    sales = pd.read_csv('data/sales.csv', parse_dates=['Дата'])
    returns = pd.read_csv('data/returns.csv', parse_dates=['Дата_возврата'])
    promotions = pd.read_csv('data/promotions.csv', parse_dates=['Дата_начала', 'Дата_окончания'], dayfirst=True)
    holidays = pd.read_csv('data/holidays.csv', parse_dates=['Дата'])

    ###############################################################################
    sales = sales.head(100_000)  # Ограничить размер для теста
    ###############################################################################

    print(f"[{time.time() - t0:.1f}s] Основные файлы загружены.")
    promotions['Это_уценка'] = clean_column_to_int(promotions['Это_уценка'])
    print(f"[{time.time() - t0:.1f}s] 'Это_уценка' очищена.")

    valid_guids = set(sales['GUID_продажи'])
    returns = returns[returns['GUID_продажи'].isin(valid_guids)].copy()
    returns_agg = returns.groupby(['GUID_продажи', 'SKU', 'Магазин'], observed=False).agg(
        Количество_возвращено=('Количество_возвращено', 'sum')
    ).reset_index()

    sales = pd.merge(sales, returns_agg, on=['GUID_продажи', 'SKU', 'Магазин'], how='left')
    sales['Количество_возвращено'] = sales['Количество_возвращено'].fillna(0)
    sales['Чистые_продажи'] = sales['Количество'] - sales['Количество_возвращено']

    sales['Сумма_чека'] = sales.groupby('GUID_продажи', observed=False)['Цена_со_скидкой'].transform('sum')
    negative_prices = sales[sales['Цена_со_скидкой'] < 0].groupby('GUID_продажи', observed=False)['Цена_со_скидкой'].sum()
    sales['Сумма_сертификата'] = sales['GUID_продажи'].map(negative_prices).fillna(0).abs()

    sales = pd.merge(sales, holidays[['Дата', 'Название_праздника', 'Тип_праздника', 'Выходной']], on='Дата', how='left')
    sales['Праздник'] = sales['Название_праздника'].notnull().astype('int8')
    sales['Праздник_тип'] = sales['Тип_праздника'].fillna('Нет')
    sales['Выходной_день'] = sales['Выходной'].fillna(0).astype('int8')
    sales.drop(columns=['Название_праздника', 'Тип_праздника', 'Выходной'], inplace=True)

    sales = pd.merge(sales, promotions, on='Номер_акции', how='left')
    sales['Акция_активна'] = (
        (sales['Номер_акции'] != 0)
        & (sales['Дата'] >= sales['Дата_начала']) 
        & (sales['Дата'] <= sales['Дата_окончания'])
    ).astype('int8')
    sales['Тип_акции'] = sales['Тип_акции'].fillna('Нет акции')
    sales['Процент_скидки'] = sales['Процент_скидки'].fillna(0)
    if sales['Это_уценка'].isna().any():
        print("Пропуски найдены в 'Это_уценка', они будут заполнены 0.")
    sales['Это_уценка'] = sales['Это_уценка'].fillna(0).astype('int8')
    sales['Промо_код_применён'] = sales['Промо_код'].notnull().astype('int8')
    sales.drop(columns=['Дата_начала', 'Дата_окончания', 'Промо_код'], inplace=True)

    sales = sales.sort_values(by=['SKU', 'Магазин', 'Дата'])
    sales = sales.astype({col: 'float32' for col in sales.select_dtypes('float64').columns})
    sales = sales.astype({col: 'int32' for col in sales.select_dtypes('int64').columns if col not in ['Магазин', 'SKU']})
    sales['SKU'] = sales['SKU'].astype(str).astype('category')
    sales.drop(columns=['GUID_продажи'], inplace=True, errors='ignore')

    print(f"=== DEBUG: Данные загружены. Размер DataFrame: {sales.shape} / время: {time.time() - t0:.1f}s ===")
    return sales, holidays, promotions

def add_advanced_features(df, holidays_df, promotions_df):
    """
    Добавляет расширенные признаки в датафрейм продаж.
    ВНИМАНИЕ: функция оптимизирована по скорости и памяти.
    """
    import time
    t0 = time.time()
    print("\n--- DEBUG: Начало расширенного создания признаков ---")

    # Скидка и флаги акций
    df['Скидка_фактическая'] = 1 - (df['Цена_со_скидкой'] / df['Цена_без_скидки'].replace(0, np.nan))
    df['Скидка_фактическая'] = df['Скидка_фактическая'].fillna(0).clip(0, 1)
    df['Была_ли_скидка'] = (df['Цена_со_скидкой'] < df['Цена_без_скидки']).astype('int8')

    promo_typemap = promotions_df.set_index('Номер_акции')['Тип_акции'].to_dict()
    clearance_map = promotions_df.set_index('Номер_акции')['Это_уценка'].to_dict()
    df['Тип_акции_расширенный'] = df['Номер_акции'].map(promo_typemap).fillna('Нет акции')
    df['Является_уценкой'] = df['Номер_акции'].map(clearance_map).fillna(0).astype('int8')

    
    df = df.sort_values(['SKU', 'Магазин', 'Дата'])
    group = df.groupby(['SKU', 'Магазин'], observed=False)

    # Скользящие признаки по продажам
    for w in [3, 7, 30]:
        print(f"  [add_advanced_features] Rolling: window={w}")
        t1 = time.time()
        df[f'Rolling_sum_{w}'] = group['Чистые_продажи'].transform(lambda x: x.rolling(w, min_periods=1).sum())
        df[f'Rolling_mean_{w}'] = group['Чистые_продажи'].transform(lambda x: x.rolling(w, min_periods=1).mean())
        df[f'Rolling_median_{w}'] = group['Чистые_продажи'].transform(lambda x: x.rolling(w, min_periods=1).median())
        df[f'Rolling_count_{w}'] = group['Чистые_продажи'].transform(lambda x: x.rolling(w, min_periods=1).count())
        print(f"    ...done in {time.time()-t1:.1f}s")

    # Признаки по праздникам
    holidays_set = set(pd.to_datetime(holidays_df['Дата']))
    print("  [add_advanced_features] Считаем признаки близости к праздникам...")
    df['Дней_до_праздника'] = df['Дата'].apply(
        lambda x: min([(h - x).days for h in holidays_set if h >= x] + [999])
    )
    df['Дней_после_праздника'] = df['Дата'].apply(
        lambda x: min([(x - h).days for h in holidays_set if h <= x] + [999])
    )

    print("  [add_advanced_features] Считаем уникальные акции за 30 дней (ускоренный способ)...")
    # Быстрый подсчет уникальных акций за 30 дней для каждой группы через rolling window
    df['Дата'] = pd.to_datetime(df['Дата'])
    df = df.sort_values(['SKU', 'Магазин', 'Дата']).copy()
    df.set_index('Дата', inplace=True)
    df['Акций_за_30д'] = (
        df.groupby(['SKU', 'Магазин'], group_keys=False, observed=False)['Номер_акции']
        .transform(lambda s: s.rolling('30D').apply(lambda x: x.nunique(), raw=False))
        .fillna(0)
        .astype('int16')
    )
    df.reset_index(inplace=True)

    # Флаги по весовому и карте клиента (исправление ошибок с типами)
    if 'Весовой' in df.columns:
        try:
            df['Весовой'] = df['Весовой'].fillna(0).astype('int8')
        except Exception:
            df['Весовой'] = df['Весовой'].fillna(0).astype('int32')
    else:
        df['Весовой'] = 0

    if 'Карта_клиента' in df.columns:
        # Сначала чистим: заменяем пустые строки и строки из пробелов на np.nan
        df['Карта_клиента'] = df['Карта_клиента'].replace(r'^\s*$', np.nan, regex=True)
        df['Карта_клиента'] = pd.to_numeric(df['Карта_клиента'], errors='coerce').fillna(0)
        # Если значения большие — int32, иначе int8
        try:
            df['Карта_клиента'] = df['Карта_клиента'].astype('int32')
        except OverflowError:
            df['Карта_клиента'] = df['Карта_клиента'].astype(str)
    else:
        df['Карта_клиента'] = 0

    print(f"--- DEBUG: Завершено расширенное создание признаков за {time.time()-t0:.1f}s ---")
    return df

def create_lags_vectorized(df, lags=[1, 7, 14, 30, 90], target_col='Чистые_продажи'):
    t0 = time.time()
    group = df.groupby(['SKU', 'Магазин'], observed=False)[target_col]
    for lag in lags:
        print(f"  [create_lags_vectorized] Lag {lag}")
        df[f'Lag_{lag}'] = group.shift(lag)
        df[f'Lag_{lag}'] = df[f'Lag_{lag}'].fillna(group.transform('median'))
    print(f"  [create_lags_vectorized] Все лаги рассчитаны за {time.time()-t0:.1f}s")
    return df

def create_rolling_vectorized(df, windows=[7, 30, 90], target_col='Чистые_продажи'):
    t0 = time.time()
    df.sort_values(by=['SKU', 'Магазин', 'Дата'], inplace=True)
    group = df.groupby(['SKU', 'Магазин'], observed=False)[target_col]
    for window in windows:
        print(f"  [create_rolling_vectorized] Скользящее среднее {window}")
        df[f'MA_{window}'] = group.transform(lambda x: x.rolling(window, min_periods=1).mean())
    print(f"  [create_rolling_vectorized] Все скользящие средние рассчитаны за {time.time()-t0:.1f}s")
    return df

def create_features_optimized(df, holidays_df, promotions_df):
    """
    Создает временные, лаговые, скользящие и расширенные признаки для модели.
    """
    import gc
    import time
    t0 = time.time()
    print("\n=== DEBUG: Начало создания временных признаков ===")
    df['День_недели'] = df['Дата'].dt.dayofweek.astype('int8')
    df['Месяц'] = df['Дата'].dt.month.astype('int8')
    df['Год'] = df['Дата'].dt.year.astype('int16')
    df['Выходной'] = (df['День_недели'] >= 5).astype('int8')
    df['Дни_с_начала'] = (df['Дата'] - df['Дата'].min()).dt.days.astype('int32')
    df['День_года'] = df['Дата'].dt.dayofyear.astype('int16')
    df['Sin_День'] = np.sin(2 * np.pi * df['День_года'] / 365).astype('float32')
    df['Cos_День'] = np.cos(2 * np.pi * df['День_года'] / 365).astype('float32')
    print(f"  [create_features_optimized] Временные признаки созданы за {time.time()-t0:.1f}s")

    print("=== DEBUG: Начало расчета лаговых признаков ===")
    df = create_lags_vectorized(df)
    print("=== DEBUG: Лаговые признаки созданы ===")

    print("=== DEBUG: Начало расчета скользящих средних ===")
    df = create_rolling_vectorized(df)
    print("=== DEBUG: Скользящие средние созданы ===")

    print("=== DEBUG: Начало расширенного feature engineering ===")
    df = add_advanced_features(df, holidays_df, promotions_df)

    print("=== DEBUG: Начало расчета трэндов и среднего по товару ===")
    df['Trend_1_7'] = (df['Lag_1'] - df['MA_7']).astype('float32')
    df['SKU_mean'] = df.groupby('SKU', observed=False)['Чистые_продажи'].transform('mean').astype('float32')
    print("=== DEBUG: Расчет трэндов и среднего по товару закончены ===")

    print("=== DEBUG: Дополнительные признаки добавлены ===")
    df['Цена_отклонение'] = (
        df['Цена_со_скидкой'] - df.groupby('SKU', observed=False)['Цена_со_скидкой'].transform('mean')
    ).astype('float32')

    print("=== DEBUG: Применение лог-преобразования к целевому признаку ===")
    df['log_Чистые_продажи'] = np.log1p(df['Чистые_продажи'].clip(lower=0))

    upper_limit = df['Чистые_продажи'].quantile(0.99)
    df['Чистые_продажи'] = df['Чистые_продажи'].clip(0, upper_limit)
    df['Чистые_продажи'] = df['Чистые_продажи'].clip(lower=0)

    gc.collect()
    print(f"=== DEBUG: Создание признаков завершено за {time.time()-t0:.1f}s ===")
    return df

# ================================================
# 4. Расширенный список признаков и категориальных для модели
# ================================================
features = [
    # Временные и базовые
    'Дни_с_начала', 'Год', 'День_недели', 'Месяц', 'Выходной', 'Праздник',
    'Праздник_тип', 'Выходной_день', 'День_года', 'Sin_День', 'Cos_День',
    # Лаги и скользящие средние
    'Lag_1', 'Lag_7', 'Lag_14', 'Lag_30', 'Lag_90',
    'MA_7', 'MA_30', 'MA_90', 'Trend_1_7', 'SKU_mean',
    # Цены и скидки
    'Цена_со_скидкой', 'Цена_отклонение', 'Процент_скидки', 'Сумма_сертификата', 'Сумма_чека',
    'Скидка_фактическая', 'Была_ли_скидка',
    # Акции и промо
    'Акция_активна', 'Тип_акции', 'Тип_акции_расширенный', 'Является_уценкой', 'Промо_код_применён',
    # Агрегации по продажам
    'Rolling_sum_3', 'Rolling_mean_3', 'Rolling_median_3', 'Rolling_count_3',
    'Rolling_sum_7', 'Rolling_mean_7', 'Rolling_median_7', 'Rolling_count_7',
    'Rolling_sum_30', 'Rolling_mean_30', 'Rolling_median_30', 'Rolling_count_30',
    'Акций_за_30д',
    # Праздничные фичи
    'Дней_до_праздника', 'Дней_после_праздника',
    # Прочие
    'Магазин', 'SKU', 'Весовой', 'Карта_клиента'
]

categorical_features = [
    'Магазин',
    'Тип_акции',
    'Тип_акции_расширенный',
    'Праздник_тип',
    'Является_уценкой',
    'Промо_код_применён',
    'Весовой',
    'Карта_клиента',
    'SKU'  # SKU теперь как категория!
]

# ================================================
# 5. Обработка категориальных признаков перед обучением
# ================================================
def prepare_categoricals(data, categorical_features):
    for col in categorical_features:
        if col in data.columns:
            data[col] = data[col].astype('category')
    return data

# ================================================
# 6. Обучение модели
# ================================================
def train_model_optuna(data, features, categorical_features, n_trials=50, random_state=42):
    """
    Обучение модели LightGBM с подбором гиперпараметров через Optuna.
    """

    import optuna

    target = 'Чистые_продажи'
    data = data.replace([np.inf, -np.inf], np.nan).dropna(subset=features + [target])

    # Разделение train/test по времени (последние 30 дней — тест)
    split_date = data['Дата'].max() - pd.Timedelta(days=30)
    train = data[data['Дата'] < split_date].copy()
    test = data[data['Дата'] >= split_date].copy()

    def objective(trial):
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
            'num_leaves': trial.suggest_int('num_leaves', 20, 128),
            'max_depth': trial.suggest_int('max_depth', 4, 15),
            'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': random_state,
            'n_jobs': 31
        }
        tscv = TimeSeriesSplit(n_splits=3)
        maes = []
        X = train[features]
        y = train[target]
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)
            dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)
            model = lgb.train(
                params, dtrain,
                valid_sets=[dval],
                num_boost_round=1000,
                callbacks=[lgb.early_stopping(50, verbose=False),
                           lgb.log_evaluation(period=100)]
            )
            preds = model.predict(X_val, num_iteration=model.best_iteration)
            maes.append(mean_absolute_error(y_val, preds))
        return np.mean(maes)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)
    print("Лучшие параметры:", study.best_params)

    best_params = study.best_params
    best_params.update({
        'objective': 'regression',
        'metric': 'mae',
        'verbose': -1,
        'random_state': random_state,
        'n_jobs': 31
    })

    # Финальное обучение на train
    lgb_train = lgb.Dataset(train[features], label=train[target], categorical_feature=categorical_features)
    lgb_test = lgb.Dataset(test[features], label=test[target])
    model = lgb.train(
        best_params,
        lgb_train,
        num_boost_round=1000,
        valid_sets=[lgb_test],
        callbacks=[lgb.early_stopping(50, verbose=False),
                   lgb.log_evaluation(period=100)]        
    )
    preds = model.predict(test[features], num_iteration=model.best_iteration)
    mae = mean_absolute_error(test[target], preds)
    print(f"MAE на тесте: {mae:.4f}")

    return model, best_params, mae, test, preds

# ================================================
# 7. Сохранение модели
# ================================================
def save_model(model, data, features, categorical_features, model_path='sales_lgbm_gpt_model.pkl'):
    """
    Сохраняет LightGBM модель и метаинформацию для последующего использования.
    """
    # Категориальные маппинги (для восстановления при инференсе)
    category_maps = {
        col: dict(enumerate(data[col].cat.categories))
        for col in categorical_features if col in data.columns and hasattr(data[col], 'cat')
    }
    joblib.dump({
        'model': model,
        'features': features,
        'categorical_features': categorical_features,
        'category_maps': category_maps
    }, model_path)
    print(f"Модель и признаки сохранены в файл: {model_path}")

def prepare_categoricals_for_inference(df, category_maps, categorical_features):
    """
    Приводит категориальные признаки к тем же категориям, что были при обучении.
    """
    for col in categorical_features:
        if col in df.columns and col in category_maps:
            # Приводим к категориям с тем же порядком
            df[col] = pd.Categorical(df[col], categories=list(category_maps[col].values()))
    return df

def predict_from_dataframe(df, model_path='sales_lgbm_gpt_model.pkl'):
    """
    Делает предсказание для датафрейма df с признаками, аналогичными обучению.
    Возвращает массив предсказаний.
    """
    # Загрузка модели и метаинформации
    model_data = joblib.load(model_path)
    model = model_data['model']
    features = model_data['features']
    categorical_features = model_data['categorical_features']
    category_maps = model_data['category_maps']

    # Подготовка категориальных признаков
    df = prepare_categoricals_for_inference(df, category_maps, categorical_features)
    # Заполняем возможные пропуски в нужных признаках
    for col in features:
        if col not in df.columns:
            df[col] = 0
    # Порядок признаков
    df = df[features]

    preds = model.predict(df, num_iteration=getattr(model, 'best_iteration', None))
    # Если предсказываем продажи — всегда >= 0
    preds = np.clip(preds, 0, None)
    return preds

def predict_from_csv(input_csv, output_csv='predictions.csv', model_path='sales_lgbm_gpt_model.pkl'):
    """
    Массовое предсказание: читает данные из CSV (с нужными признаками), делает предсказания, сохраняет результат.
    """
    df = pd.read_csv(input_csv)
    preds = predict_from_dataframe(df, model_path=model_path)
    df['Прогноз'] = preds
    df.to_csv(output_csv, index=False)
    print(f"Результаты сохранены в {output_csv}")

def predict_single_case(feature_dict, model_path='sales_lgbm_gpt_model.pkl'):
    """
    Предсказание для одного случая: подайте dict признаков как на обучении.
    """
    df = pd.DataFrame([feature_dict])
    preds = predict_from_dataframe(df, model_path=model_path)
    return preds[0]

def prepare_inference_features(
    case,
    sales_df,
    holidays_df,
    promotions_df,
    model_path="sales_lgbm_gpt_model.pkl",
    history_window=90,
    min_date="2020-01-01"
):
    """
    Формирует фичи для предсказания по одному кейсу, используя историю из sales_df.
    case: dict с минимумом ('SKU', 'Магазин', 'Дата', 'Цена_со_скидкой', ...)
    sales_df: DataFrame с полной историей продаж
    holidays_df, promotions_df: как на обучении
    model_path: путь к модели (для загрузки списка нужных признаков)
    Возвращает: DataFrame с одной строкой, готовой для predict_from_dataframe
    """
    # Загрузка метаинформации (features/categorical_features)
    model_data = joblib.load(model_path)
    features = model_data['features']
    categorical_features = model_data['categorical_features']
    category_maps = model_data['category_maps']

    sku = str(case['SKU'])
    shop = str(case['Магазин'])
    date_pred = pd.to_datetime(case['Дата'])

    # 1. Вытаскиваем хвост истории по SKU и магазину
    history = sales_df[
        (sales_df['SKU'] == sku) &
        (sales_df['Магазин'] == shop) &
        (sales_df['Дата'] < date_pred) &
        (sales_df['Дата'] >= date_pred - pd.Timedelta(days=history_window))
    ].copy()

    # 2. Добавляем день прогноза (case_row)
    # Заполняем все ключевые поля, остальные - по желанию или 0
    case_row = {
        'SKU': sku,
        'Магазин': shop,
        'Дата': date_pred,
        'Цена_со_скидкой': case.get('Цена_со_скидкой', 0),
        'Цена_без_скидки': case.get('Цена_без_скидки', case.get('Цена_со_скидкой', 0)),
        'Номер_акции': case.get('Номер_акции', 0),
        'Процент_скидки': case.get('Процент_скидки', 0),
        'Промо_код': case.get('Промо_код', np.nan)
        # Добавь другие поля, если нужно!
    }
    df_full = pd.concat([history, pd.DataFrame([case_row])], ignore_index=True)

    # 3. Генерируем признаки (как на обучении)
    # ВАЖНО: используем тот же пайплайн!
    df_full = create_features_optimized(df_full, holidays_df, promotions_df)

    # 4. Приводим категориальные (как на инференсе)
    for col in categorical_features:
        if col in df_full.columns and col in category_maps:
            df_full[col] = pd.Categorical(df_full[col], categories=list(category_maps[col].values()))

    # 5. Оставляем только день прогноза
    result_row = df_full[df_full['Дата'] == date_pred].copy()
    # Обеспечиваем нужный порядок и наличие всех фичей (для совместимости с моделью)
    for f in features:
        if f not in result_row.columns:
            result_row[f] = 0
    result_row = result_row[features]
    return result_row

def batch_predict_cases(
    cases,
    sales_df,
    holidays_df,
    promotions_df,
    model_path="sales_lgbm_gpt_model.pkl",
    history_window=90,
    min_date="2020-01-01"
):
    """
    cases: список dict-ов (SKU, Магазин, Дата, Цена_со_скидкой, ...)
    sales_df: история продаж (полный df)
    Возвращает: DataFrame с исходными кейсами и колонкой 'Прогноз'
    """
    results = []
    for case in cases:
        row = prepare_inference_features(
            case,
            sales_df,
            holidays_df,
            promotions_df,
            model_path=model_path,
            history_window=history_window,
            min_date=min_date
        )
        results.append(row)
    df_ready = pd.concat(results, ignore_index=True)
    # Предсказание
    preds = predict_from_dataframe(df_ready, model_path=model_path)
    # Собираем результат
    df_cases = pd.DataFrame(cases)
    df_cases['Прогноз'] = preds
    return df_cases

# ================================================
# 6. Пример основного пайплайна
# ================================================
if __name__ == '__main__':
    print("Загрузка и обработка данных...")
    # 1. Загрузка и подготовка данных для обучения
    data, holidays, promotions = load_data()
    data = create_features_optimized(data, holidays, promotions)
    data = prepare_categoricals(data, categorical_features)

    # 2. Обучение модели
    # model, best_params, mae, test, preds = train_model_optuna(
    #     data, features, categorical_features, n_trials=1
    # )
    # save_model(model, data, features, categorical_features, 'sales_lgbm_gpt_model.pkl')

    ###############################################################################
    # ПРИМЕР ИНФЕРЕНСА С ИСТОРИЕЙ: прогноз на будущее с учетом реальных rolling/lag
    ###############################################################################
    # 3. Загрузка истории для прогноза (тот же sales.csv, как и для обучения)
    sales_history = data.copy()  # или pd.read_csv('data/sales.csv', parse_dates=['Дата']), если нужно

    # 4. Список кейсов для предсказания
    cases = [
        {
            "SKU": 373467,
            "Магазин": "E14",
            "Дата": "2024-12-29",
            "Цена_со_скидкой": 14.63,
        },
        {
            # Факт 5шт
            "SKU": 369314,
            "Магазин": "E14",
            "Дата": "2025-04-11",
            "Цена_со_скидкой": 15.00,
        },
        {
            # Факт 4,82 кг
            "SKU": 356244,
            "Магазин": "B90",
            "Дата": "2025-04-11",
            "Цена_со_скидкой": 69.90,
        },
        {
            # Факт 3,1кг
            "SKU": 395419,
            "Магазин": "B90",
            "Дата": "2025-04-11",
            "Цена_со_скидкой": 69.90,
        }
    ]

    # 5. БАТЧ-ПРОГНОЗ с учетом истории (rolling/lag features живые!)
    df_preds = batch_predict_cases(
        cases,
        sales_history,
        holidays,
        promotions,
        model_path="sales_lgbm_gpt_model.pkl",
        history_window=90,
        min_date="2020-01-01"
    )

    # 6. Красивый вывод результатов
    for i, row in df_preds.iterrows():
        print(
            f"SKU={row['SKU']} Магазин={row['Магазин']} Дата={row['Дата']} -> Прогноз={row['Прогноз']:.2f}"
        )

    
